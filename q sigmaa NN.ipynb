{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9914a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "class n_step_replay_buffer(object):\n",
    "    def __init__(self, capacity, n_step, gamma):\n",
    "        self.capacity = capacity\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "        self.n_step_buffer = deque(maxlen=self.n_step)\n",
    "\n",
    "    def _get_n_step_info(self):\n",
    "        reward, next_observation, done = self.n_step_buffer[-1][-3:]\n",
    "        for _, _, rew, next_obs, do in reversed(list(self.n_step_buffer)[: -1]):\n",
    "            reward = self.gamma * reward * (1 - do) + rew\n",
    "            next_observation, done = (next_obs, do) if do else (next_observation, done)\n",
    "        return reward, next_observation, done\n",
    "\n",
    "    def store(self, observation, action, reward, next_observation, done):\n",
    "        observation = np.expand_dims(observation, 0)\n",
    "        next_observation = np.expand_dims(next_observation, 0)\n",
    "\n",
    "        self.n_step_buffer.append([observation, action, reward, next_observation, done])\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return\n",
    "        reward, next_observation, done = self._get_n_step_info()\n",
    "        observation, action = self.n_step_buffer[0][: 2]\n",
    "        self.memory.append([observation, action, reward, next_observation, done])\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        observation, action, reward, next_observation, done = zip(* batch)\n",
    "        return np.concatenate(observation, 0), action, reward, np.concatenate(next_observation, 0), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf81573",
   "metadata": {},
   "source": [
    "https://github.com/deligentfool/dqn_zoo/blob/master/N_step%20DQN/n_step_dqn.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f402f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ddqn(nn.Module):\n",
    "    def __init__(self, observation_dim, action_dim):\n",
    "        super(ddqn, self).__init__()\n",
    "        self.observation_dim = observation_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(self.observation_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, self.action_dim)\n",
    "\n",
    "    def forward(self, observation):\n",
    "        x = F.relu(self.fc1(observation))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def actB(self, observation, epsilon = 0.3):\n",
    "        A = np.ones(self.action_dim, dtype=float) * (epsilon/self.action_dim)\n",
    "        temp = self.forward(observation)\n",
    "        best_action = temp.max(1)[1].data[0].item()\n",
    "        A[best_action] += 1.0 - epsilon\n",
    "       \n",
    "        return A\n",
    "    \n",
    "    def actPi(self, observation, epsilon= 0.1):\n",
    "        A = np.ones(self.action_dim, dtype=float) * (epsilon/self.action_dim)\n",
    "        temp = self.forward(observation)\n",
    "        best_action = temp.max(1)[1].data[0].item()\n",
    "        A[best_action] += 1.0 - epsilon\n",
    "       \n",
    "        return A\n",
    "\n",
    "def train(buffer, target_model, eval_model, gamma, optimizer, batch_size, loss_fn, count, soft_update_freq, n_step):\n",
    "    observation, action, reward, next_observation, done = buffer.sample(batch_size)\n",
    "\n",
    "    observation = torch.FloatTensor(observation)\n",
    "    action = torch.LongTensor(action)\n",
    "    reward = torch.FloatTensor(reward)\n",
    "    next_observation = torch.FloatTensor(next_observation)\n",
    "    done = torch.FloatTensor(done)\n",
    "\n",
    "    q_values = eval_model.forward(observation)\n",
    "    next_q_values = target_model.forward(next_observation)\n",
    "    argmax_actions = eval_model.forward(next_observation).max(1)[1].detach()\n",
    "    next_q_value = next_q_values.gather(1, argmax_actions.unsqueeze(1)).squeeze(1)\n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    expected_q_value = reward + (gamma ** n_step) * (1 - done) * next_q_value\n",
    "\n",
    "    #loss = loss_fn(q_value, expected_q_value.detach())\n",
    "    loss = (expected_q_value.detach() - q_value).pow(2)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if count % soft_update_freq == 0:\n",
    "        target_model.load_state_dict(eval_model.state_dict())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54846a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9b19bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1  reward: 13.0  weight_reward: 13.000\n",
      "episode: 2  reward: 9.0  weight_reward: 12.960\n",
      "episode: 3  reward: 10.0  weight_reward: 12.930\n",
      "episode: 4  reward: 12.0  weight_reward: 12.921\n",
      "episode: 5  reward: 11.0  weight_reward: 12.902\n",
      "episode: 6  reward: 11.0  weight_reward: 12.883\n",
      "episode: 7  reward: 10.0  weight_reward: 12.854\n",
      "episode: 8  reward: 12.0  weight_reward: 12.845\n",
      "episode: 9  reward: 11.0  weight_reward: 12.827\n",
      "episode: 10  reward: 12.0  weight_reward: 12.819\n",
      "episode: 11  reward: 9.0  weight_reward: 12.781\n",
      "episode: 12  reward: 9.0  weight_reward: 12.743\n",
      "episode: 13  reward: 12.0  weight_reward: 12.735\n",
      "episode: 14  reward: 13.0  weight_reward: 12.738\n",
      "episode: 15  reward: 12.0  weight_reward: 12.731\n",
      "episode: 16  reward: 10.0  weight_reward: 12.703\n",
      "episode: 17  reward: 12.0  weight_reward: 12.696\n",
      "episode: 18  reward: 16.0  weight_reward: 12.729\n",
      "episode: 19  reward: 11.0  weight_reward: 12.712\n",
      "episode: 20  reward: 9.0  weight_reward: 12.675\n",
      "episode: 21  reward: 10.0  weight_reward: 12.648\n",
      "episode: 22  reward: 12.0  weight_reward: 12.642\n",
      "episode: 23  reward: 11.0  weight_reward: 12.625\n",
      "episode: 24  reward: 11.0  weight_reward: 12.609\n",
      "episode: 25  reward: 11.0  weight_reward: 12.593\n",
      "episode: 26  reward: 13.0  weight_reward: 12.597\n",
      "episode: 27  reward: 10.0  weight_reward: 12.571\n",
      "episode: 28  reward: 13.0  weight_reward: 12.575\n",
      "episode: 29  reward: 12.0  weight_reward: 12.570\n",
      "episode: 30  reward: 12.0  weight_reward: 12.564\n",
      "episode: 31  reward: 10.0  weight_reward: 12.538\n",
      "episode: 32  reward: 9.0  weight_reward: 12.503\n",
      "episode: 33  reward: 12.0  weight_reward: 12.498\n",
      "episode: 34  reward: 13.0  weight_reward: 12.503\n",
      "episode: 35  reward: 10.0  weight_reward: 12.478\n",
      "episode: 36  reward: 12.0  weight_reward: 12.473\n",
      "episode: 37  reward: 11.0  weight_reward: 12.458\n",
      "episode: 38  reward: 12.0  weight_reward: 12.454\n",
      "episode: 39  reward: 10.0  weight_reward: 12.429\n",
      "episode: 40  reward: 12.0  weight_reward: 12.425\n",
      "episode: 41  reward: 9.0  weight_reward: 12.391\n",
      "episode: 42  reward: 9.0  weight_reward: 12.357\n",
      "episode: 43  reward: 8.0  weight_reward: 12.313\n",
      "episode: 44  reward: 10.0  weight_reward: 12.290\n",
      "episode: 45  reward: 10.0  weight_reward: 12.267\n",
      "episode: 46  reward: 12.0  weight_reward: 12.264\n",
      "episode: 47  reward: 11.0  weight_reward: 12.252\n",
      "episode: 48  reward: 13.0  weight_reward: 12.259\n",
      "episode: 49  reward: 17.0  weight_reward: 12.307\n",
      "episode: 50  reward: 13.0  weight_reward: 12.314\n",
      "episode: 51  reward: 14.0  weight_reward: 12.330\n",
      "episode: 52  reward: 10.0  weight_reward: 12.307\n",
      "episode: 53  reward: 12.0  weight_reward: 12.304\n",
      "episode: 54  reward: 11.0  weight_reward: 12.291\n",
      "episode: 55  reward: 8.0  weight_reward: 12.248\n",
      "episode: 56  reward: 10.0  weight_reward: 12.226\n",
      "episode: 57  reward: 12.0  weight_reward: 12.223\n",
      "episode: 58  reward: 9.0  weight_reward: 12.191\n",
      "episode: 59  reward: 9.0  weight_reward: 12.159\n",
      "episode: 60  reward: 21.0  weight_reward: 12.248\n",
      "episode: 61  reward: 10.0  weight_reward: 12.225\n",
      "episode: 62  reward: 20.0  weight_reward: 12.303\n",
      "episode: 63  reward: 11.0  weight_reward: 12.290\n",
      "episode: 64  reward: 10.0  weight_reward: 12.267\n",
      "episode: 65  reward: 15.0  weight_reward: 12.294\n",
      "episode: 66  reward: 12.0  weight_reward: 12.291\n",
      "episode: 67  reward: 10.0  weight_reward: 12.269\n",
      "episode: 68  reward: 10.0  weight_reward: 12.246\n",
      "episode: 69  reward: 13.0  weight_reward: 12.253\n",
      "episode: 70  reward: 9.0  weight_reward: 12.221\n",
      "episode: 71  reward: 8.0  weight_reward: 12.179\n",
      "episode: 72  reward: 14.0  weight_reward: 12.197\n",
      "episode: 73  reward: 11.0  weight_reward: 12.185\n",
      "episode: 74  reward: 11.0  weight_reward: 12.173\n",
      "episode: 75  reward: 9.0  weight_reward: 12.141\n",
      "episode: 76  reward: 12.0  weight_reward: 12.140\n",
      "episode: 77  reward: 10.0  weight_reward: 12.118\n",
      "episode: 78  reward: 14.0  weight_reward: 12.137\n",
      "episode: 79  reward: 9.0  weight_reward: 12.106\n",
      "episode: 80  reward: 12.0  weight_reward: 12.105\n",
      "episode: 81  reward: 10.0  weight_reward: 12.084\n",
      "episode: 82  reward: 11.0  weight_reward: 12.073\n",
      "episode: 83  reward: 9.0  weight_reward: 12.042\n",
      "episode: 84  reward: 9.0  weight_reward: 12.012\n",
      "episode: 85  reward: 12.0  weight_reward: 12.012\n",
      "episode: 86  reward: 11.0  weight_reward: 12.002\n",
      "episode: 87  reward: 14.0  weight_reward: 12.022\n",
      "episode: 88  reward: 10.0  weight_reward: 12.001\n",
      "episode: 89  reward: 9.0  weight_reward: 11.971\n",
      "episode: 90  reward: 12.0  weight_reward: 11.972\n",
      "episode: 91  reward: 15.0  weight_reward: 12.002\n",
      "episode: 92  reward: 9.0  weight_reward: 11.972\n",
      "episode: 93  reward: 14.0  weight_reward: 11.992\n",
      "episode: 94  reward: 16.0  weight_reward: 12.032\n",
      "episode: 95  reward: 11.0  weight_reward: 12.022\n",
      "episode: 96  reward: 10.0  weight_reward: 12.002\n",
      "episode: 97  reward: 10.0  weight_reward: 11.982\n",
      "episode: 98  reward: 13.0  weight_reward: 11.992\n",
      "episode: 99  reward: 11.0  weight_reward: 11.982\n",
      "episode: 100  reward: 17.0  weight_reward: 12.032\n",
      "episode: 101  reward: 10.0  weight_reward: 12.012\n",
      "episode: 102  reward: 8.0  weight_reward: 11.972\n",
      "episode: 103  reward: 10.0  weight_reward: 11.952\n",
      "episode: 104  reward: 15.0  weight_reward: 11.982\n",
      "episode: 105  reward: 10.0  weight_reward: 11.963\n",
      "episode: 106  reward: 9.0  weight_reward: 11.933\n",
      "episode: 107  reward: 11.0  weight_reward: 11.924\n",
      "episode: 108  reward: 13.0  weight_reward: 11.934\n",
      "episode: 109  reward: 12.0  weight_reward: 11.935\n",
      "episode: 110  reward: 18.0  weight_reward: 11.996\n",
      "episode: 111  reward: 10.0  weight_reward: 11.976\n",
      "episode: 112  reward: 12.0  weight_reward: 11.976\n",
      "episode: 113  reward: 14.0  weight_reward: 11.996\n",
      "episode: 114  reward: 9.0  weight_reward: 11.966\n",
      "episode: 115  reward: 14.0  weight_reward: 11.987\n",
      "episode: 116  reward: 33.0  weight_reward: 12.197\n",
      "episode: 117  reward: 34.0  weight_reward: 12.415\n",
      "episode: 118  reward: 14.0  weight_reward: 12.431\n",
      "episode: 119  reward: 32.0  weight_reward: 12.626\n",
      "episode: 120  reward: 21.0  weight_reward: 12.710\n",
      "episode: 121  reward: 17.0  weight_reward: 12.753\n",
      "episode: 122  reward: 14.0  weight_reward: 12.765\n",
      "episode: 123  reward: 15.0  weight_reward: 12.788\n",
      "episode: 124  reward: 14.0  weight_reward: 12.800\n",
      "episode: 125  reward: 22.0  weight_reward: 12.892\n",
      "episode: 126  reward: 26.0  weight_reward: 13.023\n",
      "episode: 127  reward: 20.0  weight_reward: 13.093\n",
      "episode: 128  reward: 17.0  weight_reward: 13.132\n",
      "episode: 129  reward: 14.0  weight_reward: 13.141\n",
      "episode: 130  reward: 18.0  weight_reward: 13.189\n",
      "episode: 131  reward: 16.0  weight_reward: 13.217\n",
      "episode: 132  reward: 12.0  weight_reward: 13.205\n",
      "episode: 133  reward: 28.0  weight_reward: 13.353\n",
      "episode: 134  reward: 27.0  weight_reward: 13.489\n",
      "episode: 135  reward: 11.0  weight_reward: 13.465\n",
      "episode: 136  reward: 36.0  weight_reward: 13.690\n",
      "episode: 137  reward: 16.0  weight_reward: 13.713\n",
      "episode: 138  reward: 28.0  weight_reward: 13.856\n",
      "episode: 139  reward: 23.0  weight_reward: 13.947\n",
      "episode: 140  reward: 81.0  weight_reward: 14.618\n",
      "episode: 141  reward: 15.0  weight_reward: 14.622\n",
      "episode: 142  reward: 28.0  weight_reward: 14.755\n",
      "episode: 143  reward: 20.0  weight_reward: 14.808\n",
      "episode: 144  reward: 37.0  weight_reward: 15.030\n",
      "episode: 145  reward: 21.0  weight_reward: 15.090\n",
      "episode: 146  reward: 27.0  weight_reward: 15.209\n",
      "episode: 147  reward: 42.0  weight_reward: 15.477\n",
      "episode: 148  reward: 21.0  weight_reward: 15.532\n",
      "episode: 149  reward: 20.0  weight_reward: 15.576\n",
      "episode: 150  reward: 59.0  weight_reward: 16.011\n",
      "episode: 151  reward: 46.0  weight_reward: 16.311\n",
      "episode: 152  reward: 19.0  weight_reward: 16.338\n",
      "episode: 153  reward: 23.0  weight_reward: 16.404\n",
      "episode: 154  reward: 47.0  weight_reward: 16.710\n",
      "episode: 155  reward: 20.0  weight_reward: 16.743\n",
      "episode: 156  reward: 42.0  weight_reward: 16.996\n",
      "episode: 157  reward: 39.0  weight_reward: 17.216\n",
      "episode: 158  reward: 73.0  weight_reward: 17.773\n",
      "episode: 159  reward: 59.0  weight_reward: 18.186\n",
      "episode: 160  reward: 78.0  weight_reward: 18.784\n",
      "episode: 161  reward: 106.0  weight_reward: 19.656\n",
      "episode: 162  reward: 34.0  weight_reward: 19.799\n",
      "episode: 163  reward: 79.0  weight_reward: 20.391\n",
      "episode: 164  reward: 95.0  weight_reward: 21.138\n",
      "episode: 165  reward: 77.0  weight_reward: 21.696\n",
      "episode: 166  reward: 98.0  weight_reward: 22.459\n",
      "episode: 167  reward: 38.0  weight_reward: 22.615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 168  reward: 94.0  weight_reward: 23.328\n",
      "episode: 169  reward: 90.0  weight_reward: 23.995\n",
      "episode: 170  reward: 96.0  weight_reward: 24.715\n",
      "episode: 171  reward: 102.0  weight_reward: 25.488\n",
      "episode: 172  reward: 90.0  weight_reward: 26.133\n",
      "episode: 173  reward: 117.0  weight_reward: 27.042\n",
      "episode: 174  reward: 103.0  weight_reward: 27.801\n",
      "episode: 175  reward: 41.0  weight_reward: 27.933\n",
      "episode: 176  reward: 95.0  weight_reward: 28.604\n",
      "episode: 177  reward: 106.0  weight_reward: 29.378\n",
      "episode: 178  reward: 125.0  weight_reward: 30.334\n",
      "episode: 179  reward: 26.0  weight_reward: 30.291\n",
      "episode: 180  reward: 27.0  weight_reward: 30.258\n",
      "episode: 181  reward: 98.0  weight_reward: 30.935\n",
      "episode: 182  reward: 114.0  weight_reward: 31.766\n",
      "episode: 183  reward: 80.0  weight_reward: 32.248\n",
      "episode: 184  reward: 128.0  weight_reward: 33.206\n",
      "episode: 185  reward: 94.0  weight_reward: 33.814\n",
      "episode: 186  reward: 97.0  weight_reward: 34.446\n",
      "episode: 187  reward: 108.0  weight_reward: 35.181\n",
      "episode: 188  reward: 118.0  weight_reward: 36.009\n",
      "episode: 189  reward: 106.0  weight_reward: 36.709\n",
      "episode: 190  reward: 112.0  weight_reward: 37.462\n",
      "episode: 191  reward: 127.0  weight_reward: 38.358\n",
      "episode: 192  reward: 126.0  weight_reward: 39.234\n",
      "episode: 193  reward: 116.0  weight_reward: 40.002\n",
      "episode: 194  reward: 132.0  weight_reward: 40.922\n",
      "episode: 195  reward: 132.0  weight_reward: 41.833\n",
      "episode: 196  reward: 64.0  weight_reward: 42.054\n",
      "episode: 197  reward: 124.0  weight_reward: 42.874\n",
      "episode: 198  reward: 119.0  weight_reward: 43.635\n",
      "episode: 199  reward: 106.0  weight_reward: 44.259\n",
      "episode: 200  reward: 141.0  weight_reward: 45.226\n",
      "episode: 201  reward: 123.0  weight_reward: 46.004\n",
      "episode: 202  reward: 127.0  weight_reward: 46.814\n",
      "episode: 203  reward: 137.0  weight_reward: 47.716\n",
      "episode: 204  reward: 84.0  weight_reward: 48.078\n",
      "episode: 205  reward: 141.0  weight_reward: 49.008\n",
      "episode: 206  reward: 119.0  weight_reward: 49.708\n",
      "episode: 207  reward: 133.0  weight_reward: 50.540\n",
      "episode: 208  reward: 30.0  weight_reward: 50.335\n",
      "episode: 209  reward: 147.0  weight_reward: 51.302\n",
      "episode: 210  reward: 140.0  weight_reward: 52.189\n",
      "episode: 211  reward: 122.0  weight_reward: 52.887\n",
      "episode: 212  reward: 53.0  weight_reward: 52.888\n",
      "episode: 213  reward: 129.0  weight_reward: 53.649\n",
      "episode: 214  reward: 120.0  weight_reward: 54.313\n",
      "episode: 215  reward: 131.0  weight_reward: 55.079\n",
      "episode: 216  reward: 129.0  weight_reward: 55.819\n",
      "episode: 217  reward: 150.0  weight_reward: 56.760\n",
      "episode: 218  reward: 133.0  weight_reward: 57.523\n",
      "episode: 219  reward: 90.0  weight_reward: 57.848\n",
      "episode: 220  reward: 142.0  weight_reward: 58.689\n",
      "episode: 221  reward: 130.0  weight_reward: 59.402\n",
      "episode: 222  reward: 134.0  weight_reward: 60.148\n",
      "episode: 223  reward: 133.0  weight_reward: 60.877\n",
      "episode: 224  reward: 132.0  weight_reward: 61.588\n",
      "episode: 225  reward: 140.0  weight_reward: 62.372\n",
      "episode: 226  reward: 164.0  weight_reward: 63.388\n",
      "episode: 227  reward: 61.0  weight_reward: 63.364\n",
      "episode: 228  reward: 47.0  weight_reward: 63.201\n",
      "episode: 229  reward: 135.0  weight_reward: 63.919\n",
      "episode: 230  reward: 132.0  weight_reward: 64.600\n",
      "episode: 231  reward: 135.0  weight_reward: 65.304\n",
      "episode: 232  reward: 136.0  weight_reward: 66.011\n",
      "episode: 233  reward: 147.0  weight_reward: 66.821\n",
      "episode: 234  reward: 21.0  weight_reward: 66.362\n",
      "episode: 235  reward: 154.0  weight_reward: 67.239\n",
      "episode: 236  reward: 137.0  weight_reward: 67.936\n",
      "episode: 237  reward: 132.0  weight_reward: 68.577\n",
      "episode: 238  reward: 137.0  weight_reward: 69.261\n",
      "episode: 239  reward: 139.0  weight_reward: 69.959\n",
      "episode: 240  reward: 133.0  weight_reward: 70.589\n",
      "episode: 241  reward: 158.0  weight_reward: 71.463\n",
      "episode: 242  reward: 133.0  weight_reward: 72.078\n",
      "episode: 243  reward: 148.0  weight_reward: 72.838\n",
      "episode: 244  reward: 140.0  weight_reward: 73.509\n",
      "episode: 245  reward: 27.0  weight_reward: 73.044\n",
      "episode: 246  reward: 187.0  weight_reward: 74.184\n",
      "episode: 247  reward: 135.0  weight_reward: 74.792\n",
      "episode: 248  reward: 142.0  weight_reward: 75.464\n",
      "episode: 249  reward: 144.0  weight_reward: 76.149\n",
      "episode: 250  reward: 143.0  weight_reward: 76.818\n",
      "episode: 251  reward: 133.0  weight_reward: 77.380\n",
      "episode: 252  reward: 107.0  weight_reward: 77.676\n",
      "episode: 253  reward: 130.0  weight_reward: 78.199\n",
      "episode: 254  reward: 144.0  weight_reward: 78.857\n",
      "episode: 255  reward: 144.0  weight_reward: 79.509\n",
      "episode: 256  reward: 133.0  weight_reward: 80.043\n",
      "episode: 257  reward: 145.0  weight_reward: 80.693\n",
      "episode: 258  reward: 136.0  weight_reward: 81.246\n",
      "episode: 259  reward: 37.0  weight_reward: 80.804\n",
      "episode: 260  reward: 126.0  weight_reward: 81.256\n",
      "episode: 261  reward: 148.0  weight_reward: 81.923\n",
      "episode: 262  reward: 32.0  weight_reward: 81.424\n",
      "episode: 263  reward: 141.0  weight_reward: 82.020\n",
      "episode: 264  reward: 104.0  weight_reward: 82.239\n",
      "episode: 265  reward: 149.0  weight_reward: 82.907\n",
      "episode: 266  reward: 126.0  weight_reward: 83.338\n",
      "episode: 267  reward: 151.0  weight_reward: 84.015\n",
      "episode: 268  reward: 144.0  weight_reward: 84.614\n",
      "episode: 269  reward: 128.0  weight_reward: 85.048\n",
      "episode: 270  reward: 155.0  weight_reward: 85.748\n",
      "episode: 271  reward: 154.0  weight_reward: 86.430\n",
      "episode: 272  reward: 152.0  weight_reward: 87.086\n",
      "episode: 273  reward: 29.0  weight_reward: 86.505\n",
      "episode: 274  reward: 126.0  weight_reward: 86.900\n",
      "episode: 275  reward: 148.0  weight_reward: 87.511\n",
      "episode: 276  reward: 144.0  weight_reward: 88.076\n",
      "episode: 277  reward: 171.0  weight_reward: 88.905\n",
      "episode: 278  reward: 129.0  weight_reward: 89.306\n",
      "episode: 279  reward: 129.0  weight_reward: 89.703\n",
      "episode: 280  reward: 140.0  weight_reward: 90.206\n",
      "episode: 281  reward: 127.0  weight_reward: 90.574\n",
      "episode: 282  reward: 157.0  weight_reward: 91.238\n",
      "episode: 283  reward: 37.0  weight_reward: 90.696\n",
      "episode: 284  reward: 127.0  weight_reward: 91.059\n",
      "episode: 285  reward: 161.0  weight_reward: 91.758\n",
      "episode: 286  reward: 160.0  weight_reward: 92.441\n",
      "episode: 287  reward: 218.0  weight_reward: 93.696\n",
      "episode: 288  reward: 117.0  weight_reward: 93.929\n",
      "episode: 289  reward: 136.0  weight_reward: 94.350\n",
      "episode: 290  reward: 109.0  weight_reward: 94.497\n",
      "episode: 291  reward: 87.0  weight_reward: 94.422\n",
      "episode: 292  reward: 80.0  weight_reward: 94.277\n",
      "episode: 293  reward: 142.0  weight_reward: 94.755\n",
      "episode: 294  reward: 198.0  weight_reward: 95.787\n",
      "episode: 295  reward: 154.0  weight_reward: 96.369\n",
      "episode: 296  reward: 168.0  weight_reward: 97.086\n",
      "episode: 297  reward: 163.0  weight_reward: 97.745\n",
      "episode: 298  reward: 146.0  weight_reward: 98.227\n",
      "episode: 299  reward: 148.0  weight_reward: 98.725\n",
      "episode: 300  reward: 196.0  weight_reward: 99.698\n",
      "episode: 301  reward: 146.0  weight_reward: 100.161\n",
      "episode: 302  reward: 146.0  weight_reward: 100.619\n",
      "episode: 303  reward: 172.0  weight_reward: 101.333\n",
      "episode: 304  reward: 158.0  weight_reward: 101.900\n",
      "episode: 305  reward: 153.0  weight_reward: 102.411\n",
      "episode: 306  reward: 190.0  weight_reward: 103.286\n",
      "episode: 307  reward: 175.0  weight_reward: 104.004\n",
      "episode: 308  reward: 141.0  weight_reward: 104.374\n",
      "episode: 309  reward: 182.0  weight_reward: 105.150\n",
      "episode: 310  reward: 170.0  weight_reward: 105.798\n",
      "episode: 311  reward: 169.0  weight_reward: 106.430\n",
      "episode: 312  reward: 151.0  weight_reward: 106.876\n",
      "episode: 313  reward: 183.0  weight_reward: 107.637\n",
      "episode: 314  reward: 187.0  weight_reward: 108.431\n",
      "episode: 315  reward: 228.0  weight_reward: 109.627\n",
      "episode: 316  reward: 236.0  weight_reward: 110.890\n",
      "episode: 317  reward: 176.0  weight_reward: 111.541\n",
      "episode: 318  reward: 200.0  weight_reward: 112.426\n",
      "episode: 319  reward: 141.0  weight_reward: 112.712\n",
      "episode: 320  reward: 187.0  weight_reward: 113.455\n",
      "episode: 321  reward: 302.0  weight_reward: 115.340\n",
      "episode: 322  reward: 299.0  weight_reward: 117.177\n",
      "episode: 323  reward: 163.0  weight_reward: 117.635\n",
      "episode: 324  reward: 243.0  weight_reward: 118.889\n",
      "episode: 325  reward: 265.0  weight_reward: 120.350\n",
      "episode: 326  reward: 321.0  weight_reward: 122.356\n",
      "episode: 327  reward: 268.0  weight_reward: 123.813\n",
      "episode: 328  reward: 436.0  weight_reward: 126.935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 329  reward: 408.0  weight_reward: 129.745\n",
      "episode: 330  reward: 482.0  weight_reward: 133.268\n",
      "episode: 331  reward: 552.0  weight_reward: 137.455\n",
      "episode: 332  reward: 1650.0  weight_reward: 152.581\n",
      "episode: 333  reward: 691.0  weight_reward: 157.965\n",
      "episode: 334  reward: 228.0  weight_reward: 158.665\n",
      "episode: 335  reward: 64.0  weight_reward: 157.718\n",
      "episode: 336  reward: 2978.0  weight_reward: 185.921\n",
      "episode: 337  reward: 177.0  weight_reward: 185.832\n",
      "episode: 338  reward: 216.0  weight_reward: 186.134\n",
      "episode: 339  reward: 627.0  weight_reward: 190.542\n",
      "episode: 340  reward: 356.0  weight_reward: 192.197\n",
      "episode: 341  reward: 299.0  weight_reward: 193.265\n",
      "episode: 342  reward: 528.0  weight_reward: 196.612\n",
      "episode: 343  reward: 1621.0  weight_reward: 210.856\n",
      "episode: 344  reward: 1866.0  weight_reward: 227.408\n",
      "episode: 345  reward: 253.0  weight_reward: 227.664\n",
      "episode: 346  reward: 113.0  weight_reward: 226.517\n",
      "episode: 347  reward: 225.0  weight_reward: 226.502\n",
      "episode: 348  reward: 736.0  weight_reward: 231.597\n",
      "episode: 349  reward: 3043.0  weight_reward: 259.711\n",
      "episode: 350  reward: 194.0  weight_reward: 259.054\n",
      "episode: 351  reward: 59.0  weight_reward: 257.053\n",
      "episode: 352  reward: 590.0  weight_reward: 260.383\n",
      "episode: 353  reward: 154.0  weight_reward: 259.319\n",
      "episode: 354  reward: 202.0  weight_reward: 258.746\n",
      "episode: 355  reward: 600.0  weight_reward: 262.158\n",
      "episode: 356  reward: 460.0  weight_reward: 264.137\n",
      "episode: 357  reward: 696.0  weight_reward: 268.455\n",
      "episode: 358  reward: 1928.0  weight_reward: 285.051\n",
      "episode: 359  reward: 286.0  weight_reward: 285.060\n",
      "episode: 360  reward: 1108.0  weight_reward: 293.290\n",
      "episode: 361  reward: 34.0  weight_reward: 290.697\n",
      "episode: 362  reward: 848.0  weight_reward: 296.270\n",
      "episode: 363  reward: 109.0  weight_reward: 294.397\n",
      "episode: 364  reward: 620.0  weight_reward: 297.653\n",
      "episode: 365  reward: 522.0  weight_reward: 299.896\n",
      "episode: 366  reward: 383.0  weight_reward: 300.727\n",
      "episode: 367  reward: 298.0  weight_reward: 300.700\n",
      "episode: 368  reward: 1123.0  weight_reward: 308.923\n",
      "episode: 369  reward: 19.0  weight_reward: 306.024\n",
      "episode: 370  reward: 2271.0  weight_reward: 325.674\n",
      "episode: 371  reward: 504.0  weight_reward: 327.457\n",
      "episode: 372  reward: 737.0  weight_reward: 331.552\n",
      "episode: 373  reward: 28.0  weight_reward: 328.517\n",
      "episode: 374  reward: 22.0  weight_reward: 325.452\n",
      "episode: 375  reward: 100.0  weight_reward: 323.197\n",
      "episode: 376  reward: 395.0  weight_reward: 323.915\n",
      "episode: 377  reward: 502.0  weight_reward: 325.696\n",
      "episode: 378  reward: 168.0  weight_reward: 324.119\n",
      "episode: 379  reward: 184.0  weight_reward: 322.718\n",
      "episode: 380  reward: 498.0  weight_reward: 324.471\n",
      "episode: 381  reward: 186.0  weight_reward: 323.086\n",
      "episode: 382  reward: 1036.0  weight_reward: 330.215\n",
      "episode: 383  reward: 46.0  weight_reward: 327.373\n",
      "episode: 384  reward: 474.0  weight_reward: 328.839\n",
      "episode: 385  reward: 36.0  weight_reward: 325.911\n",
      "episode: 386  reward: 397.0  weight_reward: 326.622\n",
      "episode: 387  reward: 101.0  weight_reward: 324.366\n",
      "episode: 388  reward: 336.0  weight_reward: 324.482\n",
      "episode: 389  reward: 102.0  weight_reward: 322.257\n",
      "episode: 390  reward: 444.0  weight_reward: 323.475\n",
      "episode: 391  reward: 224.0  weight_reward: 322.480\n",
      "episode: 392  reward: 38.0  weight_reward: 319.635\n",
      "episode: 393  reward: 84.0  weight_reward: 317.279\n",
      "episode: 394  reward: 15.0  weight_reward: 314.256\n",
      "episode: 395  reward: 304.0  weight_reward: 314.153\n",
      "episode: 396  reward: 509.0  weight_reward: 316.102\n",
      "episode: 397  reward: 16.0  weight_reward: 313.101\n",
      "episode: 398  reward: 1147.0  weight_reward: 321.440\n",
      "episode: 399  reward: 867.0  weight_reward: 326.895\n",
      "episode: 400  reward: 331.0  weight_reward: 326.936\n",
      "episode: 401  reward: 300.0  weight_reward: 326.667\n",
      "episode: 402  reward: 92.0  weight_reward: 324.320\n",
      "episode: 403  reward: 566.0  weight_reward: 326.737\n",
      "episode: 404  reward: 706.0  weight_reward: 330.530\n",
      "episode: 405  reward: 1066.0  weight_reward: 337.884\n",
      "episode: 406  reward: 381.0  weight_reward: 338.316\n",
      "episode: 407  reward: 350.0  weight_reward: 338.432\n",
      "episode: 408  reward: 69.0  weight_reward: 335.738\n",
      "episode: 409  reward: 984.0  weight_reward: 342.221\n",
      "episode: 410  reward: 985.0  weight_reward: 348.649\n",
      "episode: 411  reward: 189.0  weight_reward: 347.052\n",
      "episode: 412  reward: 2015.0  weight_reward: 363.732\n",
      "episode: 413  reward: 63.0  weight_reward: 360.724\n",
      "episode: 414  reward: 578.0  weight_reward: 362.897\n",
      "episode: 415  reward: 512.0  weight_reward: 364.388\n",
      "episode: 416  reward: 805.0  weight_reward: 368.794\n",
      "episode: 417  reward: 589.0  weight_reward: 370.996\n",
      "episode: 418  reward: 295.0  weight_reward: 370.236\n",
      "episode: 419  reward: 16.0  weight_reward: 366.694\n",
      "episode: 420  reward: 361.0  weight_reward: 366.637\n",
      "episode: 421  reward: 230.0  weight_reward: 365.271\n",
      "episode: 422  reward: 65.0  weight_reward: 362.268\n",
      "episode: 423  reward: 735.0  weight_reward: 365.995\n",
      "episode: 424  reward: 279.0  weight_reward: 365.125\n",
      "episode: 425  reward: 428.0  weight_reward: 365.754\n",
      "episode: 426  reward: 106.0  weight_reward: 363.156\n",
      "episode: 427  reward: 502.0  weight_reward: 364.545\n",
      "episode: 428  reward: 332.0  weight_reward: 364.219\n",
      "episode: 429  reward: 70.0  weight_reward: 361.277\n",
      "episode: 430  reward: 1044.0  weight_reward: 368.104\n",
      "episode: 431  reward: 617.0  weight_reward: 370.593\n",
      "episode: 432  reward: 1626.0  weight_reward: 383.147\n",
      "episode: 433  reward: 69.0  weight_reward: 380.006\n",
      "episode: 434  reward: 321.0  weight_reward: 379.416\n",
      "episode: 435  reward: 910.0  weight_reward: 384.722\n",
      "episode: 436  reward: 87.0  weight_reward: 381.745\n",
      "episode: 437  reward: 20.0  weight_reward: 378.127\n",
      "episode: 438  reward: 62.0  weight_reward: 374.966\n",
      "episode: 439  reward: 1502.0  weight_reward: 386.236\n",
      "episode: 440  reward: 831.0  weight_reward: 390.684\n",
      "episode: 441  reward: 495.0  weight_reward: 391.727\n",
      "episode: 442  reward: 39.0  weight_reward: 388.200\n",
      "episode: 443  reward: 569.0  weight_reward: 390.008\n",
      "episode: 444  reward: 435.0  weight_reward: 390.458\n",
      "episode: 445  reward: 757.0  weight_reward: 394.123\n",
      "episode: 446  reward: 170.0  weight_reward: 391.882\n",
      "episode: 447  reward: 47.0  weight_reward: 388.433\n",
      "episode: 448  reward: 452.0  weight_reward: 389.069\n",
      "episode: 449  reward: 105.0  weight_reward: 386.228\n",
      "episode: 450  reward: 610.0  weight_reward: 388.466\n",
      "episode: 451  reward: 153.0  weight_reward: 386.111\n",
      "episode: 452  reward: 196.0  weight_reward: 384.210\n",
      "episode: 453  reward: 305.0  weight_reward: 383.418\n",
      "episode: 454  reward: 44.0  weight_reward: 380.024\n",
      "episode: 455  reward: 140.0  weight_reward: 377.623\n",
      "episode: 456  reward: 181.0  weight_reward: 375.657\n",
      "episode: 457  reward: 38.0  weight_reward: 372.281\n",
      "episode: 458  reward: 597.0  weight_reward: 374.528\n",
      "episode: 459  reward: 309.0  weight_reward: 373.873\n",
      "episode: 460  reward: 29.0  weight_reward: 370.424\n",
      "episode: 461  reward: 80.0  weight_reward: 367.520\n",
      "episode: 462  reward: 93.0  weight_reward: 364.774\n",
      "episode: 463  reward: 565.0  weight_reward: 366.777\n",
      "episode: 464  reward: 569.0  weight_reward: 368.799\n",
      "episode: 465  reward: 184.0  weight_reward: 366.951\n",
      "episode: 466  reward: 591.0  weight_reward: 369.191\n",
      "episode: 467  reward: 227.0  weight_reward: 367.769\n",
      "episode: 468  reward: 272.0  weight_reward: 366.812\n",
      "episode: 469  reward: 472.0  weight_reward: 367.864\n",
      "episode: 470  reward: 820.0  weight_reward: 372.385\n",
      "episode: 471  reward: 702.0  weight_reward: 375.681\n",
      "episode: 472  reward: 207.0  weight_reward: 373.994\n",
      "episode: 473  reward: 71.0  weight_reward: 370.964\n",
      "episode: 474  reward: 1416.0  weight_reward: 381.415\n",
      "episode: 475  reward: 357.0  weight_reward: 381.171\n",
      "episode: 476  reward: 748.0  weight_reward: 384.839\n",
      "episode: 477  reward: 61.0  weight_reward: 381.601\n",
      "episode: 478  reward: 59.0  weight_reward: 378.375\n",
      "episode: 479  reward: 501.0  weight_reward: 379.601\n",
      "episode: 480  reward: 172.0  weight_reward: 377.525\n",
      "episode: 481  reward: 153.0  weight_reward: 375.280\n",
      "episode: 482  reward: 617.0  weight_reward: 377.697\n",
      "episode: 483  reward: 320.0  weight_reward: 377.120\n",
      "episode: 484  reward: 237.0  weight_reward: 375.719\n",
      "episode: 485  reward: 107.0  weight_reward: 373.031\n",
      "episode: 486  reward: 168.0  weight_reward: 370.981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 487  reward: 287.0  weight_reward: 370.141\n",
      "episode: 488  reward: 761.0  weight_reward: 374.050\n",
      "episode: 489  reward: 209.0  weight_reward: 372.399\n",
      "episode: 490  reward: 154.0  weight_reward: 370.215\n",
      "episode: 491  reward: 139.0  weight_reward: 367.903\n",
      "episode: 492  reward: 166.0  weight_reward: 365.884\n",
      "episode: 493  reward: 150.0  weight_reward: 363.725\n",
      "episode: 494  reward: 133.0  weight_reward: 361.418\n",
      "episode: 495  reward: 142.0  weight_reward: 359.224\n",
      "episode: 496  reward: 269.0  weight_reward: 358.322\n",
      "episode: 497  reward: 841.0  weight_reward: 363.148\n",
      "episode: 498  reward: 503.0  weight_reward: 364.547\n",
      "episode: 499  reward: 206.0  weight_reward: 362.961\n",
      "episode: 500  reward: 414.0  weight_reward: 363.472\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gamma = 0.99\n",
    "    learning_rate = 1e-3\n",
    "    batch_size = 64\n",
    "    soft_update_freq = 200\n",
    "    capacity = 10000\n",
    "    exploration = 100\n",
    "    decay = 0.99\n",
    "    episode = 500\n",
    "    n_step = 4\n",
    "    render = False\n",
    "    sigma = 0.5\n",
    "    \n",
    "    env = gym.make('CartPole-v0')\n",
    "    env = env.unwrapped\n",
    "    observation_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    target_net = ddqn(observation_dim, action_dim)\n",
    "    eval_net = ddqn(observation_dim, action_dim)\n",
    "    eval_net.load_state_dict(target_net.state_dict())\n",
    "    optimizer = torch.optim.Adam(eval_net.parameters(), lr=learning_rate)\n",
    "    buffer = n_step_replay_buffer(capacity, n_step, gamma)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    count = 0\n",
    "\n",
    "    weight_reward = None\n",
    "    for i in range(episode):\n",
    "        obs = env.reset()\n",
    "        T = np.inf\n",
    "        t = 0\n",
    "        tau = 0\n",
    "        stored_actions = {}\n",
    "        stored_states = {}\n",
    "        stored_rewards = {}\n",
    "        stored_pho = {}\n",
    "        stored_sigma={}\n",
    "        reward_total = 0\n",
    "\n",
    "        b_prob = eval_net.actB(torch.FloatTensor(np.expand_dims(obs, 0)))\n",
    "        pi_prob = eval_net.actPi(torch.FloatTensor(np.expand_dims(obs, 0)))\n",
    "        action = np.random.choice(list(range(action_dim)),p=b_prob)\n",
    "        pho = pi_prob[action] / b_prob[action]\n",
    "        \n",
    "        stored_actions[0] = action\n",
    "        stored_states[0] = obs\n",
    "        stored_pho[0] = pho\n",
    "        stored_rewards[0] = 0\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        while True:\n",
    "            \n",
    "            if t < T:\n",
    "                next_obs, reward, done, info = env.step(action)\n",
    "                reward_total += reward\n",
    "                stored_states[(t+1) % (n_step+1)] = next_obs\n",
    "                stored_rewards[(t+1) % (n_step+1)] = reward\n",
    "                buffer.store(obs, action, reward, next_obs, done)\n",
    "                obs = next_obs\n",
    "                if done:\n",
    "                    T= t+1\n",
    "                else:\n",
    "                    b_prob = eval_net.actB(torch.FloatTensor(np.expand_dims(obs, 0)))\n",
    "                    pi_prob = eval_net.actPi(torch.FloatTensor(np.expand_dims(obs, 0)))\n",
    "                    action = np.random.choice(list(range(action_dim)),p=b_prob)\n",
    "                    pho = pi_prob[action] / b_prob[action]\n",
    "                    stored_actions[(t+1)% (n_step+1)] = action\n",
    "                    stored_pho[(t+1)% (n_step+1)] = pho\n",
    "                    \n",
    "            tau = t - n_step + 1\n",
    "            if tau >= 0:\n",
    "                if t + 1 < T:\n",
    "                    q_actions = torch.FloatTensor(stored_states[(t+1)% (n_step+1)])\n",
    "                    q = eval_net.forward(q_actions).detach()\n",
    "                    G = q[stored_actions[(t+1)% (n_step+1)]]\n",
    "                    \n",
    "                for k in range(min(t+1, T), tau, -1):\n",
    "                    if k == T:\n",
    "                        G = stored_rewards[T% (n_step+1)]\n",
    "                    else:\n",
    "                        s_k = stored_states[k% (n_step+1)]\n",
    "                        a_k = stored_actions[k% (n_step+1)]\n",
    "                        r_k = stored_rewards[k% (n_step+1)]\n",
    "                        pho_k = stored_pho[k% (n_step+1)]\n",
    "                        \n",
    "                        b_prob = eval_net.actB(torch.FloatTensor(np.expand_dims(s_k, 0)))\n",
    "                        pi_prob = eval_net.actPi(torch.FloatTensor(np.expand_dims(s_k, 0)))\n",
    "                        q_net = torch.FloatTensor(s_k)\n",
    "                        q = eval_net.forward(q_net).detach()\n",
    "                        \n",
    "                        VBar = np.sum([(pi_prob[a]) * q[a] for a in range(action_dim)])\n",
    "                        G = r_k + gamma * ((sigma * pho_k) + ((1-sigma) * pi_prob[a_k])) * (G - q[a_k])+ gamma * VBar\n",
    "\n",
    "                s_tau = stored_states[tau% (n_step+1)]\n",
    "                a_tau = stored_actions[tau% (n_step+1)]\n",
    "                observationQ = torch.FloatTensor(s_tau)\n",
    "                \n",
    "                q_actions = torch.FloatTensor(s_tau)\n",
    "                q = eval_net.forward(q_net).detach()\n",
    "                \n",
    "                q[a_tau] = q[a_tau] + learning_rate * (G - q[a_tau])\n",
    "         \n",
    "               \n",
    "            if i > exploration:\n",
    "                train(buffer, target_net, eval_net, gamma, optimizer, batch_size, loss_fn, count, soft_update_freq, n_step)\n",
    "            \n",
    "            if tau >= (T-1):\n",
    "                if not weight_reward:\n",
    "                    weight_reward = reward_total\n",
    "                else:\n",
    "                    weight_reward = 0.99 * weight_reward + 0.01 * reward_total\n",
    "                print('episode: {}  reward: {}  weight_reward: {:.3f}'.format(i+1, reward_total, weight_reward))\n",
    "                break\n",
    "            else:\n",
    "                t = t + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ddd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880bb909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
